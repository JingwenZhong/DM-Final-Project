# -*- coding: utf-8 -*-
"""DM - Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14lYp8sSkvTe0JWBQkqxXS19v27r0wW5E

# DSC440 Data Mining - Final Project
# Jingwen Zhong & Charlotte Ding
"""

# Importing packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objs as go
import plotly.express as px
from plotly.subplots import make_subplots 
from plotly.offline import init_notebook_mode, iplot
import re

from google.colab import files
uploaded = files.upload()

"""## Data cleaning"""

BA = pd.read_csv("BusinessAnalyst.csv")
# BA.info()

DA = pd.read_csv("DataAnalyst.csv")
# DA.info()

DE = pd.read_csv("DataEngineer.csv")
# DE.info() ## No unnamed column

DS = pd.read_csv("DataScientist.csv")
# DS.info()

a= 0 
for i in BA['Unnamed: 0']:
  if not i.isdigit():
    BA.drop(index=(BA.loc[(BA['Unnamed: 0']==i)].index), axis=0,inplace=True)

BA.drop(['Unnamed: 0','index'],axis=1,inplace=True)


BA['Job Title'],BA['Department']= BA['Job Title'].str.split(',',1).str
# BA['Company Name'],_ = BA['Company Name'].str.split('\n',1).str
# BA['Salary Estimate'],_= BA['Salary Estimate'].str.split('(',1).str

# Delete unnamed column
DA.drop(['Unnamed: 0'], axis = 1, inplace = True)

# Separate values 
DA['Job Title'],DA['Department']= DA['Job Title'].str.split(',',1).str
# DA['Company Name'],_ = DA['Company Name'].str.split('\n',1).str
# DA['Salary Estimate'],_= DA['Salary Estimate'].str.split('(',1).str

# Separate values
DE['Job Title'],DE['Department']= DE['Job Title'].str.split(',',1).str
# DE['Company Name'],_ = DE['Company Name'].str.split('\n',1).str
# DE['Salary Estimate'],_= DE['Salary Estimate'].str.split('(',1).str

# Delete unnamed column
DS.drop(['Unnamed: 0', 'index'], axis = 1, inplace = True)

# Separate values
DS['Job Title'],DS['Department']= DS['Job Title'].str.split(',',1).str
# DS['Company Name'],_ = DS['Company Name'].str.split('\n',1).str
# DS['Salary Estimate'],_= DS['Salary Estimate'].str.split('(',1).str

BA.head(3)

DA.head(3)

DE.head(3)

DS.head(3)

"""## Data Visualization

### Seperated dataset
"""

import nltk # Natural Language toolkit
nltk.download("stopwords")  #downloading stopwords
nltk.download('punkt')
from nltk import word_tokenize,sent_tokenize
nltk.download('wordnet')
import nltk as nlp

from sklearn.feature_extraction.text import CountVectorizer #Bag of Words
from wordcloud import WordCloud, STOPWORDS

max_features=500 # "number" most common(used) words in reviews
count_vectorizer=CountVectorizer(max_features=max_features,stop_words="english") # stop words will be dropped by stopwords command

top10_job_titles= BA['Job Title'].value_counts().sort_values(ascending = False).head(10)
plt.pie(top10_job_titles, autopct='%.2f',pctdistance=1.7)
plt.title('Top 10 Most Popular Job Postings for BA',fontweight ='bold')
plt.legend(list(dict(top10_job_titles).keys()), loc='upper right')
plt.show()

sparce_matrix1=count_vectorizer.fit_transform(BA['Job Description']).toarray()# this code will create matrix that consist of 0 and 1.
BA_word = pd.DataFrame(count_vectorizer.get_feature_names(), columns=["Words"])
plt.subplots(figsize=(12,12))
wordcloud = WordCloud(background_color="white",width=1024,height=768).generate(" ".join(BA_word.Words[100:]))
plt.imshow(wordcloud)
plt.title('most used word in BA job description',fontweight ='bold')
plt.axis("off")
plt.show()
# print("Top {} the most used word in business analyst job description: {}".format(max_features,count_vectorizer.get_feature_names()))

top10_job_titles= DA['Job Title'].value_counts().sort_values(ascending = False).head(10)
plt.pie(top10_job_titles, autopct='%.2f',pctdistance=1.7)
plt.title('Top 10 Most Popular Job Postings for DA',fontweight ='bold')
plt.legend(list(dict(top10_job_titles).keys()), loc='upper right')
plt.show()

sparce_matrix2 = count_vectorizer.fit_transform(DA['Job Description']).toarray()# this code will create matrix that consist of 0 and 1.
DA_word = pd.DataFrame(count_vectorizer.get_feature_names(), columns=["Words"])
wordcloud2 = WordCloud(background_color = "white",width=1024,height=768).generate(" ".join(DA_word.Words[100:]))
plt.subplots(figsize=(12,12))
plt.title('most used word in DA job description',fontweight ='bold')
plt.imshow(wordcloud2)
plt.axis("off")
plt.show()

top10_job_titles= DE['Job Title'].value_counts().sort_values(ascending = False).head(10)
plt.pie(top10_job_titles, autopct='%.2f',pctdistance=1.7)
plt.title('Top 10 Most Popular Job Postings for DE',fontweight ='bold')
plt.legend(list(dict(top10_job_titles).keys()), loc='upper right')
plt.show()

sparce_matrix3 = count_vectorizer.fit_transform(DE['Job Description']).toarray()# this code will create matrix that consist of 0 and 1.
DE_word = pd.DataFrame(count_vectorizer.get_feature_names(), columns=["Words"])
wordcloud3 = WordCloud(background_color = "white",width=1024,height=768).generate(" ".join(DE_word.Words[100:]))
plt.subplots(figsize=(12,12))
plt.imshow(wordcloud3)
plt.title('most used word in DE job description',fontweight ='bold')
plt.axis("off")
plt.show()

top10_job_titles= DS['Job Title'].value_counts().sort_values(ascending = False).head(10)
plt.pie(top10_job_titles, autopct='%.2f',pctdistance=1.7)
plt.title('Top 10 Most Popular Job Postings for DS',fontweight ='bold')
plt.legend(list(dict(top10_job_titles).keys()), loc='upper right')
plt.show()

sparce_matrix4 = count_vectorizer.fit_transform(DS['Job Description']).toarray()# this code will create matrix that consist of 0 and 1.
DS_word = pd.DataFrame(count_vectorizer.get_feature_names(), columns=["Words"])
wordcloud4 = WordCloud(background_color = "white",width=1024,height=768).generate(" ".join(DS_word.Words[100:]))
plt.subplots(figsize=(12,12))
plt.imshow(wordcloud4)
plt.title('most used word in DS job description',fontweight ='bold')
plt.axis("off")
plt.show()

import re
# A dictionary is created with a list of hard skills to search in the job description fields
hard_skills_dict = {
    'Python': r"python",
    'R': r"[\b\s/]r[\s,\.]",
    'Excel': r"excel",
    'SQL': r"sql",
    'NoSQL': r"\bNo[\s,-]sql[\s]",
    'PowerBI': r"power[\s]BI",
    'Tableau': r"tableau",
    'SPSS': r'\bSPSS\b',
    'Big Data': r"\sbig\sdata\s",
    'SAP BI': r"SAP[\s]BI",
    'MongoDB': r"MongoDB",
    'Hadoop': r"Hadoop",
    'SAS': r"\bSAS\b",
    'VBA': r"\bvba\b",
    'AWS': r"\baws\b",
    'Git': r"\bGit",
    'QlikView': r"\bQlikView",
    'Oracle BI': r"oracle[\n]BI",
    'Scala': r"Scala",
    'Dashboard': r"\bDashboard[s]",
    'Spark': r"Spark",
    'Matlab': r"Matplotlib",
    'Linux': r"linux",
    'Unix': r"unix",
    'Looker': r"looker",
    'C# or C++': r"\bC[#\+\+]",
    'Java': r"java",
    'PowerPivot': r"Power[\s]Pivot",
    'PowerQuery': r"Power[\s]Query",
    'BigQuery': r"Big[\s]Query",
    'Apache Cassandra': r"[\b\s]Cassandra[\b\s]",
    'Neo4j': r"Neo4j",
    'TensorFlow': r"TensorFlow"
}
hard_skills_count = {}

## BA
# Loop through skills for count the frecuency in Jobs Description Field.
for key, search in hard_skills_dict.items():
    hard_skills_count[key] = BA['Job Description'].str.contains(search, flags = re.IGNORECASE).sum()

BA_skills = pd.DataFrame.from_dict(hard_skills_count, orient = 'index') \
                        .sort_values(0, ascending = False) \
                        .reset_index() \
                        .rename(columns = {'index': 'Skills', 0: 'Count'})

BA_skills['Relative Frequency'] = (BA_skills['Count'] / sum(BA_skills['Count'])) *100
BA_skills['Relative Frequency'] = BA_skills['Relative Frequency'].apply(lambda x: round(x, 2))

# Remove values less than '1' per cent in its Relative Frequency
BA_skills = BA_skills.drop(BA_skills[BA_skills['Relative Frequency'] < 1.00].index, axis = 0)

BA_chart_skills = px.bar(BA_skills, x = 'Skills', y = 'Count',
                     color = 'Relative Frequency',
                     labels = {'Skills': 'Hard Skills of BA','Count': 'Number of Requests'})
BA_chart_skills.show()
# Business related job requires Excel (office) skills.

## DA
# Loop through skills for count the frecuency in Jobs Description Field.
for key, search in hard_skills_dict.items():
    hard_skills_count[key] = DA['Job Description'].str.contains(search, flags = re.IGNORECASE).sum()

DA_skills = pd.DataFrame.from_dict(hard_skills_count, orient = 'index') \
                        .sort_values(0, ascending = False) \
                        .reset_index() \
                        .rename(columns = {'index': 'Skills', 0: 'Count'})

DA_skills['Relative Frequency'] = (DA_skills['Count'] / sum(DA_skills['Count'])) *100
DA_skills['Relative Frequency'] = DA_skills['Relative Frequency'].apply(lambda x: round(x, 2))

# Remove values less than '1' per cent in its Relative Frequency
DA_skills = DA_skills.drop(DA_skills[DA_skills['Relative Frequency'] < 1.00].index, axis = 0)

DA_chart_skills = px.bar(DA_skills, x = 'Skills', y = 'Count',
                     color = 'Relative Frequency',
                     labels = {'Skills': 'Hard Skills of DA','Count': 'Number of Requests'})
DA_chart_skills.show()

## DE
# Loop through skills for count the frecuency in Jobs Description Field.
for key, search in hard_skills_dict.items():
    hard_skills_count[key] = DE['Job Description'].str.contains(search, flags = re.IGNORECASE).sum()

DE_skills = pd.DataFrame.from_dict(hard_skills_count, orient = 'index') \
                        .sort_values(0, ascending = False) \
                        .reset_index() \
                        .rename(columns = {'index': 'Skills', 0: 'Count'})

DE_skills['Relative Frequency'] = (DE_skills['Count'] / sum(DE_skills['Count'])) *100
DE_skills['Relative Frequency'] = DE_skills['Relative Frequency'].apply(lambda x: round(x, 2))

# Remove values less than '1' per cent in its Relative Frequency
DE_skills = DE_skills.drop(DE_skills[DE_skills['Relative Frequency'] < 1.00].index, axis = 0)

DE_chart_skills = px.bar(DE_skills, x = 'Skills', y = 'Count',
                     color = 'Relative Frequency',
                     labels = {'Skills': 'Hard Skills of DE','Count': 'Number of Requests'})
DE_chart_skills.show()

## DATA SCIENCE
# Loop through skills for count the frecuency in Jobs Description Field.
for key, search in hard_skills_dict.items():
    hard_skills_count[key] = DS['Job Description'].str.contains(search, flags = re.IGNORECASE).sum()

DS_skills = pd.DataFrame.from_dict(hard_skills_count, orient = 'index') \
                        .sort_values(0, ascending = False) \
                        .reset_index() \
                        .rename(columns = {'index': 'Skills', 0: 'Count'})

DS_skills['Relative Frequency'] = (DS_skills['Count'] / sum(DS_skills['Count'])) *100
DS_skills['Relative Frequency'] = DS_skills['Relative Frequency'].apply(lambda x: round(x, 2))

# Remove values less than '1' per cent in its Relative Frequency
DS_skills = DS_skills.drop(DS_skills[DS_skills['Relative Frequency'] < 1.00].index, axis = 0)

DS_chart_skills = px.bar(DS_skills, x = 'Skills', y = 'Count',
                     color = 'Relative Frequency',
                     labels = {'Skills': 'Hard Skills of DS','Count': 'Number of Requests'})
DS_chart_skills.show()

"""#### Combined dataset"""

# Append the original job types
# 1 as BA, 2 as DA, 3 as DE, 4 as DS


Type1 = []
for i in range(len(BA)):
  Type1.append('BA')
BA.insert(1,'label', Type1)


Type2 = []
for i in range(len(DA)):
  Type2.append('DA')

DA.insert(1,'label', Type2)


Type3 = []
for i in range(len(DE)):
  Type3.append('DE')

DE.insert(1,'label', Type3)


Type4 = []
for i in range(len(DS)):
  Type4.append('DS')

DS.insert(1,'label', Type4)

# merge dataset
frames=[BA, DA, DE, DS]
data = pd.concat(frames)

# data.isnull().sum()
## 有null但不在interested的column
# data =  pd.read_excel("combined.xlsx")

pattern = re.compile(r'(\d.*?)\D')
def format_salary(string):
    try:
        result = re.search(pattern, string).group(1)
    except:
        return 0
    return int(result)
data['Salary Estimate'] = data['Salary Estimate'].map(format_salary)
## Salary取min（因为一般如果给一个range 其实真的给到手都是min （手动狗头

# fig, ax = plt.subplots(figsize = [20, 7])
# sns.histplot(data, x = "Salary Estimate", kde = True, hue = "label", fill = True, ax = ax)
# plt.show()

## 看salary小于10的
# data[data["Salary Estimate"] < 10]
## 没有job description和rating 不可用所以删除
## delete
data = data[data["Salary Estimate"] > 10]

## 剩下的数据个数
data.label.value_counts()
sns.countplot(x = "label", data = data, palette = "Set3")
plt.grid()
plt.show()
## DA & DE少很多

sns.catplot(x = "label", y = "Salary Estimate", hue = "label", bw = 0.15, cut = 0,
            kind = "violin", data = data)
plt.grid()
plt.show()
## DS和DE的工资起薪还是比BA和DA高一点

## 搞rating数据
# def format_rating(string):
#    try:
#        result = float(string)
#    except:
#        return 0
#    return result
# data['Rating'] = data['Rating'].map(format_rating)

# sns.catplot(x = "label", y = "Rating", hue = "label", kind = "box", data = data)
# plt.grid()
# plt.show()
## 得分和职业无关 所以不放进model

pattern = r'\W' ## 匹配所有字符串数字下划线
data[['Job Title','Job Description']] = data[['Job Title','Job Description']].applymap(lambda x:re.sub(pattern,' ',x))
## 所有非字母数字下划线字符 替换为空格
data[['Job Title','Job Description']] = data[['Job Title','Job Description']].applymap(lambda x:x.lower())

data.head()

"""#Model

### Training preparation
"""

from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score
from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,log_loss,precision_score
import warnings
warnings.filterwarnings("ignore")
from sklearn.metrics import roc_auc_score,roc_curve

"""#### Word to Vector"""

from tensorflow.keras.preprocessing.text import Tokenizer
import tensorflow as tf

# np.random.seed(20)

rate = [0.7, 0.5]
## 训练集占总sample size的70% 测试集占剩余的50% 剩下为验证集
## N*0.7 / N*(1-0.7)*0.5

def get_train_test_val(data, rate):
    data = data.loc[:, ['Job Title', 'Salary Estimate', 'Job Description', 'label']]
    train = data.sample(frac = rate[0])#按比例抽取训练集
    data = data[~data.index.isin(train.index)]
    test = data.sample(frac = rate[1])
    val = data[~data.index.isin(test.index)]
    return train, test, val

train, test, val = get_train_test_val(data, rate)

x_tokenizer = Tokenizer(lower = True, char_level = False, oov_token = '<UNK>')
## 未知的单词设置为'<UNK>'

vocab = train.loc[:, ['Job Title', 'Job Description']].apply(lambda x:' '.join(x),axis=1)
vocab = vocab.map(lambda x:re.sub('\s+',' ',x))## 多个空格换成一个空格
vocab

x_tokenizer.fit_on_texts(vocab)
vocab_size = len(x_tokenizer.word_index) + 1
print (f"# tokens: {vocab_size}")

def word_to_vector(data):
    data['Job Title'] = x_tokenizer.texts_to_sequences(data['Job Title'])
    data['Job Description'] = x_tokenizer.texts_to_sequences(data['Job Description'])
    return data

train = word_to_vector(train)
test = word_to_vector(test)
val = word_to_vector(val)
train.head()
## 看一下转换后的数据

def decode(indices, tokenizer):
    ## 把数字向量转文本   
    return " ".join([tokenizer.index_word[index] for index in indices])
decode(train.iloc[0,0], x_tokenizer)
## 拿job title第一行试试

## 因为四个数据样本量不一样 设置类别权重 样本少的权重相应提高 样本多的权重减少
classWeights = 1 / train.label.value_counts() * 1000
classWeights

diclabel = {k:v for v,k in enumerate(classWeights.index)}
##类别字典

train.label = train.label.map(diclabel)
train.head()

train['Job Title'].map(len).hist(bins = 100)

train['Job Description'].map(len).hist(bins = 100)

from tensorflow.keras.preprocessing.sequence import pad_sequences

## 把vector补齐到一样的长度 jobTitle不足10的用0填充
## jobDescription填充到1500

def padding(data): ## 补齐
    data['Job Title'] = data['Job Title'].map(lambda x:pad_sequences([x], padding = "post", maxlen = 10))
    data['Job Description'] = data['Job Description'].map(lambda x:pad_sequences([x], padding = "post", maxlen = 1500))
    return data

train = padding(train)
train.head()

## JobTitle, Salary, jobDescription分别作为model的三个不同输入 一起打包
def get_db(data):
    title = np.concatenate(data['Job Title'].values)
    salary = data['Salary Estimate'].values.reshape(-1,1)
    desc = np.concatenate(data['Job Description'].values)
    feature = [tf.cast(x,dtype=tf.float32) for x in [title,salary,desc]]
    label = tf.cast(data.label,dtype=tf.float32)
    return feature, label

train_x, train_y = get_db(train)

"""### Training"""

## model

title_input = tf.keras.Input(shape = [10], name = 'title')
title_embedding = tf.keras.layers.Embedding(vocab_size, 64)(title_input)
## FutureWork - 64可以改32
## 把一个word再次扩展 超越现有的维度 在3D的角度看2D的数据
title_lstm = tf.keras.layers.LSTM(32)(title_embedding)
title_output = tf.keras.layers.Dense(4, activation='softmax')(title_lstm)
## 4可以试别的


salary_input = tf.keras.Input(shape=[1], name = 'salary')
salary_dense = tf.keras.layers.Dense(10, activation = 'relu')(salary_input)
## 10可以改
salary_output = tf.keras.layers.Dense(4, activation = 'softmax')(salary_dense)
## 4可以改

desc_input = tf.keras.Input(shape=[1500], name = 'desc')
desc_embedding = tf.keras.layers.Embedding(vocab_size, 64)(desc_input)
desc_lstm = tf.keras.layers.LSTM(32)(desc_embedding)
desc_output = tf.keras.layers.Dense(4,activation = 'softmax')(desc_lstm)

concatenated = tf.keras.layers.concatenate([title_output, salary_output, desc_output], axis=-1)

output = tf.keras.layers.Dense(4, activation = 'softmax')(concatenated)

model = tf.keras.Model(inputs = [title_input, salary_input, desc_input], outputs = output)

## tf.keras.utils.plot_model(model, show_layer_names = True, show_shapes = True)

## optimizer
model.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.001),
loss = tf.keras.losses.SparseCategoricalCrossentropy(),
metrics = ['acc'])

def preprocess_data(data):
  data.label = data.label.map(diclabel)
  data = padding(data)
  feature, label = get_db(data)
  return feature,label
test_x ,test_y = preprocess_data(test)

## 开始train
history = model.fit(train_x, train_y,
          epochs = 20,## Future Work 可以改次数
          batch_size = 64, ##
          validation_data = (test_x,test_y),
          class_weight = {k:v for k,v in zip(range(4),classWeights.values)})

his = pd.DataFrame(history.history)

fig,ax = plt.subplots(1, 2, figsize= (20,10))
ax[0].plot(history.epoch, 'loss', data = his, label = 'loss')
ax[0].plot(history.epoch, 'val_loss', data = his, label = 'val_loss')
ax[0].set_xlabel('epochs')
ax[0].set_ylabel('error_value')
ax[0].set_title('loss vs epochs')
ax[0].legend()

ax[1].plot(history.epoch, 'acc',data = his,label = 'acc')
ax[1].plot(history.epoch, 'val_acc',data = his,label = 'val_acc')
ax[1].set_xlabel('epochs')
ax[1].set_ylabel('accuracy')
ax[1].set_title('acc vs epochs')
ax[1].legend()
plt.show()

## validation
val_x, val_y = preprocess_data(val)

pred = tf.argmax(model.predict(val_x), axis = 1)

confusion_matrix(val_y, pred, labels = [0,1,2,3])

sns.heatmap(confusion_matrix(val_y, pred,labels = [0,1,2,3]), annot=True, fmt='.3g')
plt.show()

model.evaluate(val_x, val_y)